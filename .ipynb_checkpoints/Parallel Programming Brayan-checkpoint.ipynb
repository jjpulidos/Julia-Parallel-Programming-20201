{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multi-Core or Distributed Processing 🥉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An implementation of distributed memory 🧠 parallel computing is provided by module **Distributed** as part of the standard library 📚 shipped with Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "using Distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Julia provides a multiprocessing environment based on message passing to allow programs to run on multiple processes in separate memory 🧠 domains at once.\n",
    "\n",
    "Communication in Julia is generally \"one-sided\", meaning that the programmer needs to explicitly manage only one process in a two-process operation. Furthermore, these operations typically do not look like \"message 📨 send\" and \"message 📨 receive\" but rather resemble higher-level operations like calls to user functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Managing worker processes 👷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Functions **addprocs**, **rmprocs**, **workers** 👷, and others are available as a programmatic means of adding, removing and querying the processes.\n",
    "\n",
    "Module **Distributed** must be explicitly loaded on the master process before invoking addprocs. It is automatically made available on the worker 👷 processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Int64,1}:\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addprocs(5) #Add 5 new worker processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Int64,1}:\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers() #Show all worker processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Int64,1}:\n",
       " 2\n",
       " 3\n",
       " 4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmprocs(6,5) #Remove the worker 6 and 5\n",
    "workers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "🧐 Distributed programming in Julia is built on two primitives: **remote references** 📚 and **remote calls** 📞. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Remote references 📚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A remote reference is an object that can be used from any process to refer to an object stored on a particular process.\n",
    "\n",
    "Remote references come in two flavors: **Future** 🔮 and **RemoteChannel** 🕳️.\n",
    "\n",
    "A remote call returns a **Future** 🔮 to its result. Remote calls return immediately; the process that made the call proceeds to its next operation while the remote call happens somewhere else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can wait for a remote call to finish by calling **wait** on the returned **Future** 🔮, and you can obtain the full value of the result using **fetch**.\n",
    "\n",
    "**RemoteChannels** 🕳️ are rewritable. For example, multiple processes can co-ordinate their processing by referencing the same remote Channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Remote calls 📞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A remote call is a request by one process to call a certain function on certain arguments on another (possibly the same) process.\n",
    "\n",
    "The first argument to **remotecall** 📞 is the function to call. Most parallel programming in Julia does not reference specific processes or the number of processes available, but **remotecall** 📞 is considered a low-level interface providing finer control 🎮. The second argument to **remotecall** 📞 is the id of the process that will do the work, and the remaining arguments will be passed to the function being called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Future(2, 1, 7, nothing)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = remotecall(rand, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Future(2, 1, 8, nothing)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = @spawnat 2 1 .+ fetch(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×4 Array{Float64,2}:\n",
       " 1.4976   1.26589  1.61851  1.26621\n",
       " 1.9398   1.7587   1.63158  1.87834\n",
       " 1.71603  1.21418  1.6479   1.06598"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As you can see, in the first line we asked process 2 to construct a 3-by-4 random 🔀 matrix, and in the second line we asked it to add 1 to it. The result of both calculations 🧮 is available in the two futures, r and s. The **@spawnat** macro evaluates the expression in the second argument on the process specified by the first argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Channels and RemoteChannels 🕳️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- A **Channel** 🕳️ is local to a process. Worker 👷 2 cannot directly refer to a Channel on worker 👷 3 and vice-versa. A **RemoteChannel** 🕳️, however, can put and take values across workers.\n",
    "\n",
    "\n",
    "- A **RemoteChannel** 🕳️ can be thought of as a handle to a **Channel** 🕳️.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "- The process id, pid, associated with a **RemoteChannel** 🕳️ identifies the process where the backing store, i.e., the backing **Channel** 🕳️ exists.\n",
    "\n",
    "\n",
    "- Any process with a reference to a **RemoteChannel** 🕳️ can put and take items from the channel. Data is automatically sent to (or retrieved from) the process a **RemoteChannel** 🕳️ is associated with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Serializing a **Channel** 🕳️ also serializes any data present in the channel. Deserializing it therefore effectively makes a copy of the original object.\n",
    "\n",
    "\n",
    "- On the other hand, serializing a **RemoteChannel** 🕳️ only involves the serialization of an identifier that identifies the location and instance of Channel referred to by the handle. A deserialized **RemoteChannel** 🕳️ object (on any worker 👷), therefore also points to the same backing store as the original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The channels example from above can be modified for interprocess communication.\n",
    "\n",
    "Jobs, identified by an id (job_id), are written to the channel. Each remotely executing task in this simulation reads a job_id, waits for a random 🔀 amount of time ⏱️ and writes back a tuple of job_id, time taken and its own pid to the results channel. Finally all the results are printed out on the master process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "const jobs = RemoteChannel(()->Channel{Int}(32));\n",
    "const results = RemoteChannel(()->Channel{Tuple}(32));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@everywhere function do_work(jobs, results) # define work function everywhere\n",
    "   while true\n",
    "       job_id = take!(jobs)\n",
    "       exec_time = rand()\n",
    "       sleep(exec_time) # simulates elapsed time doing actual work\n",
    "       put!(results, (job_id, exec_time, myid()))\n",
    "   end\n",
    "end\n",
    "\n",
    "function make_jobs(n)\n",
    "   for i in 1:n\n",
    "       put!(jobs, i)\n",
    "   end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n = 12;\n",
    "@async make_jobs(n); # feed the jobs channel with \"n\" jobs\n",
    "\n",
    "for p in workers() # start tasks on the workers to process requests in parallel\n",
    "   remote_do(do_work, p, jobs, results)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 finished in 0.06 seconds on worker 4\n",
      "1 finished in 0.13 seconds on worker 2\n",
      "4 finished in 0.03 seconds on worker 4\n",
      "5 finished in 0.11 seconds on worker 2\n",
      "6 finished in 0.26 seconds on worker 4\n",
      "2 finished in 0.5 seconds on worker 3\n",
      "8 finished in 0.5 seconds on worker 4\n",
      "10 finished in 0.02 seconds on worker 4\n",
      "7 finished in 0.87 seconds on worker 2\n",
      "9 finished in 0.77 seconds on worker 3\n",
      "11 finished in 0.72 seconds on worker 4\n",
      "12 finished in 0.76 seconds on worker 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.354414809"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@elapsed while n > 0 # print out results\n",
    "   job_id, exec_time, where = take!(results)\n",
    "   println(\"$job_id finished in $(round(exec_time; digits=2)) seconds on worker $where\")\n",
    "   global n = n - 1\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallel Map and Loops 🔁"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can use **@spawnat** to flip coins on two processes. First, write the following function in count_heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@everywhere function count_heads(n)\n",
    "    c::Int = 0\n",
    "    for i = 1:n\n",
    "        c += rand(Bool)\n",
    "    end\n",
    "    c\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.844747 seconds (83.96 k allocations: 4.359 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99995071"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sum()\n",
    "    a = @spawnat 1 count_heads(100000000)\n",
    "    b = @spawnat 2 count_heads(100000000)\n",
    "    fetch(a)+fetch(b)\n",
    "end\n",
    "@time sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.556201 seconds (1 allocation: 16 bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100005826"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time a = count_heads(200000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We used two explicit **@spawnat** statements, which limits the parallelism to two processes. To run on any number of processes, we can use a parallel for loop 🔁, running in distributed memory, which can be written in Julia using **@distributed** like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.326883 seconds (58.94 k allocations: 2.927 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99998369"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time nheads = @distributed (+) for i = 1:200000000\n",
    "    Int(rand(Bool))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This construct implements the pattern of assigning iterations to multiple processes, and combining them with a specified reduction (in this case (+)). The result of each iteration is taken as the value of the last expression inside the loop 🔁. The whole parallel loop 🔁 expression itself evaluates to the final answer.\n",
    "\n",
    "Note that although parallel for loops look like serial for loops 🔁, their behavior is dramatically different. In particular, the iterations do not happen in a specified order, and writes to variables or arrays will not be globally visible since iterations run on different processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Any variables used inside the parallel loop 🔁 will be copied and broadcast to each process.\n",
    "\n",
    "For example, the following code will not work as intended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = zeros(10)\n",
    "@distributed for i = 1:10\n",
    "    a[i] = i\n",
    "end\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This code will not initialize all of a, since each process will have a separate copy of it. Parallel for loops 🔁 like these must be avoided.\n",
    "\n",
    "Fortunately, Shared Arrays can be used to get around this limitation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element SharedArray{Float64,1}:\n",
       "  1.0\n",
       "  2.0\n",
       "  3.0\n",
       "  4.0\n",
       "  5.0\n",
       "  6.0\n",
       "  7.0\n",
       "  8.0\n",
       "  9.0\n",
       " 10.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using SharedArrays\n",
    "\n",
    "a = SharedArray{Float64}(10)\n",
    "@distributed for i = 1:10\n",
    "    a[i] = i\n",
    "end\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**SharedArray** will be explained below\n",
    "\n",
    "Using \"outside\" variables in parallel loops 🔁 is perfectly reasonable if the variables are read-only 👓:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Task (runnable) @0x00007f10bfe27340"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = randn(10)\n",
    "@distributed for i = 1:10\n",
    "    println(b[i])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Shared array 🖇️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Shared Arrays use system shared memory 🧠 to map the same array across many processes. In a **SharedArray** 🖇️ each \"participating\" process has access to the entire array. A **SharedArray** 🖇️ is a good choice when you want to have a large amount of data jointly accessible to two or more processes on the same machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The constructor for a shared array is of the form:\n",
    "``` julia\n",
    "SharedArray{T,N}(dims::NTuple; init=false, pids=Int[])\n",
    "```\n",
    "Which creates an N-dimensional shared array of a bits type T and size dims across the processes specified by pids.\n",
    "A shared array is accessible only from those participating workers specified by the pids named argument (and the creating process too, if it is on the same host).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "If an init function, of signature initfn(S::SharedArray), is specified, it is called on all the participating workers 👷. You can specify that each worker runs the init function on a distinct portion of the array, thereby parallelizing initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 2:\t0.028566486513405803\n",
      "      From worker 2:\t0.08741719073442625\n",
      "      From worker 2:\t-0.9475990413661702\n",
      "      From worker 2:\t-0.4165271807588755\n",
      "      From worker 4:\t0.7835690894410531\n",
      "      From worker 4:\t-0.14577964749827468\n",
      "      From worker 4:\t0.0858166350405183\n",
      "      From worker 3:\t0.25742333881797125\n",
      "      From worker 3:\t-1.5288476236166226\n",
      "      From worker 3:\t0.04238308914446343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3×4 SharedArray{Int64,2}:\n",
       " 2  2  3  4\n",
       " 2  3  3  4\n",
       " 2  3  4  4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@everywhere using SharedArrays\n",
    "S = SharedArray{Int,2}((3,4), init = S -> S[localindices(S)] = repeat([myid()], length(localindices(S))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**SharedArray** 🖇️ indexing (assignment and accessing values) works just as with regular arrays, and is efficient because the underlying memory 🧠 is available to the local process 👷."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×4 SharedArray{Int64,2}:\n",
       " 2  2  3  4\n",
       " 2  3  3  4\n",
       " 2  7  4  4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[3,2] = 7\n",
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since all processes have access to the underlying data, you do have to be careful not to set up conflicts. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×4 SharedArray{Int64,2}:\n",
       " 3  3  2  3\n",
       " 3  2  2  3\n",
       " 3  2  3  3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@sync begin\n",
    "    for p in procs(S)\n",
    "        @async begin\n",
    "            remotecall_wait(fill!, p, S, p)  #Perform wait(remotecall(...))\n",
    "        end\n",
    "    end\n",
    "end\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×4 SharedArray{Int64,2}:\n",
       " 3  3  2  2\n",
       " 3  2  2  2\n",
       " 3  2  2  2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@sync begin\n",
    "    for p in procs(S)\n",
    "        @async begin\n",
    "            remotecall_wait(fill!, p, S, p)  #Perform wait(remotecall(...))\n",
    "        end\n",
    "    end\n",
    "end\n",
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The parallel loops 🔁 example from above can be better understood now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element SharedArray{Float64,1}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using SharedArrays\n",
    "\n",
    "a = SharedArray{Float64}(10)\n",
    "@distributed for i = 1:10\n",
    "    a[i] = i\n",
    "end\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia (4 threads) 1.4.2",
   "language": "julia",
   "name": "julia-(4-threads)-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
